---
title: "Project1: How the public take the inaugural speeches."
author: "Yi Xiang[yx2365@columbia.edu]"
date: "January 30, 2017"
output: html_document
---

##Step0: Install and load libraries
```{r}
ptm <- proc.time()
packages.used=c("tm", "wordcloud", "RColorBrewer", 
                "dplyr", "tidytext","ggplot2","SnowballC","qdap"
                ,"data.table","scales","MASS")


# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
library(ggplot2)
library(SnowballC)
library(qdap)
library(sentimentr)
library(data.table)
library(scales)
library(MASS)
```

##Step1: Read Speeachs and save in Corpus
##please change the working directory before runing the code, current working directory is under my local drive.
```{r}
setwd("C:/Users/Elaine/Documents/Columbia/Spring_2017/AppliedDataScience/Spr2017-Proj1-eeelaine")
folder.path<-paste(getwd(),"/data/InauguralSpeeches/",sep = "")
speeches<-list.files(path=folder.path,pattern = "*.txt")
prex.out<-substr(speeches,6,stop = nchar(speeches)-4)

ff.all<-Corpus(DirSource(folder.path))

```

##Step2: Clean the data using text mining tools
```{r}
CleanCorpus<-function(mycorpus){
mycorpus<-tm_map(mycorpus, stripWhitespace)#strip unnecessary white space
mycorpus<-tm_map(mycorpus, content_transformer(tolower))#convert to lowercases
mycorpus<-tm_map(mycorpus,removeWords,stopwords("en"))#remove english stopwords
mycorpus<-tm_map(mycorpus, removeWords, character(0))
mycorpus<-tm_map(mycorpus,removePunctuation)#remove punctuations
mycorpus<-tm_map(mycorpus,stemDocument)#remove common word endings
}

ff.all<-CleanCorpus(ff.all)
```

##Step3: Calculate the TF-IDF(Term Frequency-Inverse Document frequency) weighted matrix for speeches
```{r}

dtm.all<-DocumentTermMatrix(ff.all,control=list(weighting =
                                         function(x)
                                         weightTfIdf(x, normalize =
                                                     FALSE),
                                         stopwords = TRUE))

ff.dtm<-tidy(dtm.all)
ff.matrix<-as.matrix(dtm.all)

```

##Step4: Find the most frequently used words
```{r}
## words after sparse unfamiliar words
dtms <- removeSparseTerms(dtm.all, 0.15)##remove sparse terms
freq_removedsparse <- colSums(as.matrix(dtms))   
freq_removedsparse

##15 most frequently used words
freq_all <- sort(colSums(as.matrix(dtm.all)), decreasing=TRUE)   
head(freq_all, 15) 

##words that has been used over 100 times
freq_overhundred<-findFreqTerms(dtm.all,lowfreq = 100)
freq_overhundred

##plot the terms that has frequency over 100 times
ff.dataframe<-data.frame(term=names(freq_all),count=freq_all)
p <- ggplot(subset(ff.dataframe, count>100), aes(term, count))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))  
p

ggsave(paste(getwd(),"/output/","AllSpeechesWordCloud,".png",sep = ""),plot=last_plot())
```

##Step5:Draw the wordcloud of all the speeches and also donuald trump's speech
results shows that the most frequently used words in all speeches include "freedom"."constitute","ideal","law","tariff", and in Donald Trump's speech it includes "job","back","dream","everyone". The results shows the main policy Mr.Trump is holding.
```{r}
par(mfrow=c(1,1))
##wordcloud of all the speeches
png(paste(getwd(),"/output/WordCloud/", "WordCloudAll",".png",sep = ""),
      width=300, height=300)
wordcloud(names(freq_all), freq_all, min.freq=80,
              rot.per=0,
              random.color=TRUE,
              colors=brewer.pal(10,"Greens"))
dev.off()

##trump's speech
Trump_Speech<-data.frame(term=ff.dtm$term[ff.dtm$document=="inaugDonaldJTrump-1.txt"],count=ff.dtm$count[ff.dtm$document=="inaugDonaldJTrump-1.txt"])

Trump_Speech<-Trump_Speech[-3,]

##words at least mentioned 3 times
png(paste(getwd(),"/output/WordCloud/", "TrumpWordCloud1",".png",sep = ""),
      width=300, height=300)
wordcloud(Trump_Speech$term, Trump_Speech$count, min.freq=3,
              scale=c(5,0.5),
              max.words=200,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Greens")) 

dev.off()

##words at least mentioned 10 times
png(paste(getwd(),"/output/WordCloud/", "TrumpWordCloud2",".png",sep = ""),
      width=300, height=300)
wordcloud(Trump_Speech$term, Trump_Speech$count, min.freq=10,
              scale=c(5,0.5),
              max.words=200,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Greens")) 
dev.off()

```

##Step6: Split Sentences Function
Split the speech into sentences which are used for further analysis.
```{r}
##following uncommented code help analyze the frequent word in each speech:
# speech.corpus<-Corpus(VectorSource(speech.raw))
# speech.corpus<-CleanCorpus(speech.corpus)
# speech.tidy<-tidy(speech.corpus)
# dtm.speech<-DocumentTermMatrix(speech.corpus)
# freq_terms<-findFreqTerms(dtm.speech,lowfreq = 5)

SentencesAnalysis<-function(speech.raw,president.name){
  
# Split the speech into sentences
# qdap's sentSplit is modeled after dialogue data, so person field is needed
speech.df <- data.table(speech=speech.raw, person=president.name) #change to data frame
speechcombined<-sentCombine(speech.df) #combine
speechcombined.df<-data.table(speech=speechcombined$text.var,person=president.name) #and then change to data frame
sentences<-data.table(sentSplit(speechcombined.df, "speech")) #split into sentenctes, and keep in a list name "sentences",the list name for the splited sentences is "speech", the last variable here

# Add a sentence counter and remove unnecessary variables
sentences[, sentence.num := seq(nrow(sentences))]
sentences[, person := NULL]
sentences[, tot := NULL]
setcolorder(sentences, c("sentence.num", "speech"))

# Syllables per sentence
sentences[, syllables := syllable_sum(speech)]
sentences<-na.omit(sentences)

# Add cumulative syllable count and percent complete as proxy for progression
sentences[, syllables.cumsum := cumsum(syllables)] ##get the cumulative sums of syllables
sentences[, pct.complete := syllables.cumsum / sum(sentences$syllables)] #get the percent of syllables
sentences[, pct.complete.100 := pct.complete * 100]#percentage

return(sentences)
}


```

##Step6: Sentiment Analysis Function: record the sentiment change throughout every speech
qdap's sentiment analysis is based on a sentence-level formula classifying each word as either positive, negative, neutral, negator or amplifier, per Hu & Liu's sentiment lexicon. The function also provides a word count.
```{r}
my.theme <- 
  theme(plot.background = element_blank(), # Remove background
        panel.grid.major = element_blank(), # Remove gridlines
        panel.grid.minor = element_blank(), # Remove more gridlines
        panel.border = element_blank(), # Remove border
        panel.background = element_blank(), # Remove more background
        axis.ticks = element_blank(), # Remove axis ticks
        axis.text=element_text(size=14), # Enlarge axis text font
        axis.title=element_text(size=16), # Enlarge axis title font
        plot.title=element_text(size=24, hjust=0)) # Enlarge, left-align title

CustomScatterPlot <- function(gg)
  return(gg + geom_point(color="grey60") + # Lighten dots
           stat_smooth(color="royalblue", fill="lightgray", size=1.4) + 
           xlab("Percent complete (by syllable count)") + 
           scale_x_continuous(labels = percent) + my.theme)

SentimentAnalysis<-function(sentences,president.name){
  pol.df <- polarity(sentences$speech)$all#get the datafram of polarity analysis
  sentences[, words := pol.df$wc]  #wordcount
  sentences[, pol := pol.df$polarity]  #polarity score of words
  
  ##plot and save in output folder, sentimentplots subfolder


CustomScatterPlot(ggplot(sentences, aes(pct.complete, pol)) +
                    ylab("Sentiment (sentence-level polarity)") + 
                    ggtitle(paste("Sentiment of",president.name,sep = " ")))

return(sentences)
}


```

##Step7: Readability Tests Function
Readability Tests typically based on syllables, words, and sentences in order to approximate the grade level required to comprehend a text. 
The higher the grade level stands for higher educated level.
Here the method used is automated readability index, which has the following score levels:
1	5-6	Kindergarten
2	6-7	First Grade
3	7-8	Second Grade
4	8-9	Third Grade
5	9-10	Fourth Grade
6	10-11	Fifth Grade
7	11-12	Sixth Grade
8	12-13	Seventh Grade
9	13-14	Eighth Grade
10	14-15	Ninth Grade
11	15-16	Tenth Grade
12	16-17	Eleventh grade
13	17-18	Twelfth grade
14	18-22	College
```{r}

ReadabilityAnalysis<-function(sentences,president.name){
  sentences[, readability := automated_readability_index(speech, sentence.num)
            $Readability$Automated_Readability_Index]
  
  CustomScatterPlot(ggplot(sentences, aes(pct.complete, readability)) +
                      ylab("Automated Readability Index") +
                      ggtitle(paste("Readability of",president.name,sep = " ")))
  
  return(sentences)
}

```

##Step8: Memorability Analysis
Using google search hits to indicate the public opinion about sentences in each speech, the most popular senence would naturally have highest google hits.
Here we plot the memorability of sentences throughout each speech, and also record 7 sentences with highest google hits in a csv file.
one drawback of this method is that google will block our program after 300 times search.
```{r}
GoogleHits <- function(query){
  require(XML)
  require(RCurl)
  
  url <- paste0("https://www.google.com/search?q=", gsub(" ", "+", query))
  
  CAINFO = paste0(system.file(package="RCurl"), "/CurlSSL/ca-bundle.crt")
  script <- getURL(url, followlocation=T, cainfo=CAINFO)
  doc <- htmlParse(script)
  res <- xpathSApply(doc, '//*/div[@id="resultStats"]', xmlValue)
  return(as.numeric(gsub("[^0-9]", "", res)))
}

GoogleHitsAnalysis<-function(sentences,president.name){
  
  sentences[, google.hits := GoogleHits(paste0("[", gsub("[,;!.]", "", speech), 
                                               "]"))]
  
  googlehits.sentences<-head(sentences[order(-google.hits)]$speech, 7)
  
  write.csv(googlehits.sentences,file = paste(getwd(),"/output/Memorability/HitsSentences/","HitsSentences_",president.name,".csv",sep = ""))
  
  #Plotting Google hits on a log scale reduces skew and allows us to work on a ratio scale.
  sentences[, log.google.hits := log(google.hits)]
  
  CustomScatterPlot(ggplot(sentences, aes(pct.complete, log.google.hits)) +
                      ylab("Memorability (log of sentence's Google hits)") +
                      ggtitle(paste("Memorability of",president.name,sep = " ")))
  
  return(sentences)
}

```


##Step9: Loop through all the speeches and perform Sentiment Analysis and Readability Analysis. Since google literally blocked me after trying to search 300th times using code, I choose to use DonaldTrump's speech as an example of Memorability Analysis.
this part can not be knit to html since there is a loop in saving pictures to folders, so we do not run this while kniting, instead we run it alone to generate results.
```{r}
for ( i in 1:length(speeches)){
  filename<-speeches[i]
  president.name<-substr(filename,6,nchar(filename)-4)
  speech.raw<-read.table(paste(folder.path,filename,sep = ""),quote = NULL,comment="",header = FALSE,fill = TRUE)
  sentences<-SentencesAnalysis(speech.raw,president.name)

  ##sentiment Analysis
  sentences<-SentimentAnalysis(sentences,president.name)
  ggsave(paste(getwd(),"/output/SentimentPlots/","Sentiment_",president.name,".png",sep = ""),plot=last_plot())

  ##Readability Analysis
  sentences<-ReadabilityAnalysis(sentences,president.name)
  ggsave(paste(getwd(),"/output/ReadabilityPlots/","Readability_",president.name,".png",sep = ""),plot=last_plot())
}

```

Results shows that for sentiment, most of the speech express more positive feelings at the end of the speech compmaring to the begining.
For readability, past presidents' speech require higher level of grade to understand their speech than recent presidents.

##Step10: Sentiment,Readibility and Memorability Analysis of Donald Trump's Speech
```{r}
  i=9
  filename<-speeches[i]
  president.name<-substr(filename,6,nchar(filename)-4)
  speech.raw<-read.table(paste(folder.path,filename,sep = ""),quote = NULL,comment="",header = FALSE,fill = TRUE)
  sentences<-SentencesAnalysis(speech.raw,president.name)
  
  ##sentiment Analysis
  sentences<-SentimentAnalysis(sentences,president.name)
  last_plot()
  
  ##Readability Analysis
  sentences<-ReadabilityAnalysis(sentences,president.name)
  last_plot()
  
  ##Memorability Analysis,i.e Google Hits Analysis
  sentences<-GoogleHitsAnalysis(sentences,president.name)
  ggsave(paste(getwd(),"/output/Memorability/Plots/","Memorability_",president.name,".png",sep = ""),plot=last_plot())
  last_plot()
  
  ##the most heat 7 sentences in Trump's speech
  head(sentences[order(-google.hits)]$speech, 7)
  
```

##Step11:further explore the determinants of memorability of sentences
Results shows that the higher level of reading grade, the less memorability it is for the sentences. 
```{r}
google.lm <- stepAIC(lm(log(google.hits) ~ poly(readability, 3) + pol + pct.complete.100, data=sentences))
summary(google.lm)

new.data <- data.frame(readability=seq(min(sentences$readability), 
                                       max(sentences$readability), by=0.1),
                       pct.complete.100=mean(sentences$pct.complete.100))

new.data$pred.hits <- predict(google.lm, newdata=new.data)

ggplot(new.data, aes(readability, pred.hits)) + 
  geom_line(color="royalblue", size=1.4) + 
  xlab("Automated Readability Index") +
  ylab("Predicted memorability (log Google hits)") +
  ggtitle("Predicted memorability ~ readability") +
  my.theme

ggsave(paste(getwd(),"/output/Memorability/Plots/","Memorability_readability",president.name,".png",sep = ""),plot=last_plot())

usedtime<-proc.time() - ptm
##time used in runing the program
usedtime
```

